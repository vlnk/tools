Dans cet algorithme, au lieu de modifier directement $C[i,j]$ on parallélise l'initialisation d'un tableau $R$ de $n$ pour qu'à la case $i$ il y ait le produit $A[i,k] \times B[k,j]$. Ensuite, on fait appel à une fonction récursive qui prend en paramètre $n$ et $R$, et qui retourne la valeur de $C[i,j]$. Cette fonction permet de calculer $C[i,j]$ en une durée $log \, n$,
dedans, on a simplement une base (condition d'arrêt de la récursivité) et deux appels récursifs parallèlisés avec spawn. Chaque appel récursif prend en paramètre la moitié du tableau, et lorsque le tableau n'a qu'une case, on retourne la valeur inscrite dans cette case. Après la synchronisation des deux appels récursifs, on somme leurs résultats.

\input{contents/figures/q1algo}

\cleardoublepage
\input{contents/algorithms/multiply}
\input{contents/algorithms/multiply_add}

\subsubsection*{a) la durée totale}

On a une durée totale de :
\begin{equation} \label{eq1}
\begin{split}
T_\infty(n) & = 3 \times log \, n + log \, n\\ 
 & = \Theta (log \, n)
\end{split}
\end{equation}

En effet, les $3$ premières boucles de l'algorithme sont parallélisées et s'effectuent en une durée de $3 \times log \, n$ qui correspond à la création des threads. De plus, la fonction récursive \textit{add} permet de faire l'addition en une durée de $log \, n$ .

\subsubsection*{b) le travail}

On garde le même travail que dans l'algorithme précédent avec en plus la récursion en $log \, n$.
\begin{equation} \label{eq2}
\begin{split}
T_1(n) & = n^2 \times  (n + log \, n)\\ 
 & = \Theta (n^3 + n^2 \times log \, n)\\
 & = \Theta (n^3)
\end{split}
\end{equation}

On gagne donc en durée au prix d'un travail plus grand mais non significatif. Le parallélisme est par contre plus important :
$p = \frac{n^3}{log \, n}$ dans le cas modifié contre un parallélisme $p = n^2$ dans le cas de base.